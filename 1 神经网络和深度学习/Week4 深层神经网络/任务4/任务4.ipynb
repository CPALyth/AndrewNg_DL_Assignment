{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Building your Deep Neural Network: Step by Step\n",
    "\n",
    "欢迎来到您的第4周的作业（2的第1部分）！您之前已经训练了一个2层神经网络（带有一个隐藏层）。本周，你将建立一个深层的神经网络，层数由你而定！\n",
    "\n",
    "在这个笔记本中，您将实现构建深度神经网络所需的所有功能。\n",
    "在下一个任务中，您将使用这些函数为图像分类构建深度神经网络。\n",
    "\n",
    "**完成这项任务后，您将能够：**\n",
    "\n",
    "- 使用像ReLU这样的非线性单元来改进你的模型\n",
    "- 建立一个更深层的神经网络（具有多于一个的隐藏层）\n",
    "- 实现一个易于使用的神经网络类\n",
    "\n",
    "---\n",
    "\n",
    "**Notation（符号）**:\n",
    "- 上标 $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - 例如: $a^{[L]}$ 是第 $L^{th}$ 层的激活函数. $W^{[L]}$ 和 $b^{[L]}$ 是第 $L^{th}$ 层的参数.\n",
    "- 上标 $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - 例如: $x^{(i)}$ 是第 $i^{th}$ 训练样本.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - 例如: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n",
    "    \n",
    "**Superscript:上标; Lowerscript:下标**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1 - 包\n",
    "\n",
    "我们首先导入您在这个任务中需要的所有包.\n",
    "- [numpy](www.numpy.org) 科学计算包.\n",
    "- [matplotlib](http://matplotlib.org) 绘制图表的库.\n",
    "- dnn_utils 提供了一些必须的功能.\n",
    "- testCases 提供了一些测试用例，用来评估函数的正确性."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "source": [
    "## 2 - 初始化参数\n",
    "### 2-1 两层神经网络（即单隐层）\n",
    "\n",
    "模型结构是线性->ReLU->线性->sigmod函数。  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- W1 (n_h, n_x)\n",
    "- b1 (n_h, 1)\n",
    "- W2 (n_y, n_h)\n",
    "- b2 (n_y, 1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "W1 =  [[ 0.01624345 -0.00611756]\n [-0.00528172 -0.01072969]]\nb1 =  [[0.]\n [0.]]\nW2 =  [[ 0.00865408 -0.02301539]]\nb2 =  [[0.]]\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "parameters = initialize_parameters(2,2,1)\n",
    "print(\"W1 = \", parameters[\"W1\"])\n",
    "print(\"b1 = \", parameters[\"b1\"])\n",
    "print(\"W2 = \", parameters[\"W2\"])\n",
    "print(\"b2 = \", parameters[\"b2\"])"
   ]
  },
  {
   "source": [
    "### 2-2 L层神经网络\n",
    "更深层的L层神经网络的初始化更复杂，因为有更多的权重矩阵和偏置向量\n",
    "\n",
    "当完成 `initialize_parameters_deep` 时, 你应该确保每个图层的维度是匹配的.回想下课程上所说的， $n^{[l]}$ 表示第$l$层神经元的个数. 因此假如我们输入的 $X$ 的大小是 $(12288, 209)$ (with $m=209$ examples)\n",
    "\n",
    "**Layer L-1: ** $(n^{[L-1]}, n^{[L-2]})$ $(n^{[L-1]}, 1)$ $Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ $(n^{[L-1]}, 209)$ \n",
    "\n",
    "**Layer L: ** $(n^{[L]}, n^{[L-1]})$ $(n^{[L]}, 1)$ $Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]}$ $(n^{[L]}, 209)$\n",
    "\n",
    "当我们计算 $W X + b$ 的时候, 其实利用了python的“广播”特性. 假设: \n",
    "\n",
    "$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$\n",
    "\n",
    "那么:\n",
    "\n",
    "$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters[\"b\"+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "W1 =  [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\nb1 =  [[0.]\n [0.]\n [0.]\n [0.]]\nW2 =  [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n [-0.00768836 -0.00230031  0.00745056  0.01976111]]\nb2 =  [[0.]\n [0.]\n [0.]]\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \", parameters[\"W1\"])\n",
    "print(\"b1 = \", parameters[\"b1\"])\n",
    "print(\"W2 = \", parameters[\"W2\"])\n",
    "print(\"b2 = \", parameters[\"b2\"])"
   ]
  },
  {
   "source": [
    "## 3 - 前向传播"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3-1 线性传播\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- W1 (n_h, n_x)\n",
    "- b1 (n_h, 1)\n",
    "- W2 (n_y, n_h)\n",
    "- b2 (n_y, 1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Z= [[ 3.26295337 -1.23429987]]\n"
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z=\", Z)"
   ]
  },
  {
   "source": [
    "为了方便起见，你要把这两个功能 (Linear and Activation)组合为一个功能(LINEAR->ACTIVATION)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "With sigmoid: A =  [[0.96890023 0.11013289]]\nWith ReLU: A =  [[3.43896131 0.        ]]\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"sigmoid\")\n",
    "print(\"With sigmoid: A = \", A)\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "print(\"With ReLU: A = \", A)"
   ]
  },
  {
   "source": [
    "L层模型前向传播"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "AL =  [[0.17007265 0.2524272 ]]\nLength of caches list =  2\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \", AL)\n",
    "print(\"Length of caches list = \", len(caches))"
   ]
  },
  {
   "source": [
    "太好了，你现在已经有了一个完整的向前传播，它接受输入X，并输出了一个包含你的预测的行向量 $A^{[L]}$ 。它还用“缓存”记录了所有中间值。使用 $A^{[L]}$ ，你可以计算预测结果的损失成本。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5 - 损失函数\n",
    "\n",
    "Now you will implement forward and backward propagation. 你需要计算cost，因为你想检查你的模型是否真的在学习。\n",
    "\n",
    "**Exercise**: Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -1/m * np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cost =  0.41493159961539694\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "Y, AL = compute_cost_test_case()\n",
    "print(\"cost = \", compute_cost(AL, Y))"
   ]
  },
  {
   "source": [
    "## 6 - 反射传播模型\n",
    "\n",
    "Just like with forward propagation, you will implement helper functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "与前向传播类似，我们有需要使用三个步骤来构建反向传播：\n",
    "\n",
    "- LINEAR 后向计算\n",
    "- LINEAR -> ACTIVATION 后向计算，其中ACTIVATION 计算Relu或者Sigmoid 的结果\n",
    "- [LINEAR -> RELU] × \\times× (L-1) -> LINEAR -> SIGMOID 后向计算 (整个模型)\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dA_prev =  [[ 0.51822968 -0.19517421]\n [-0.40506361  0.15255393]\n [ 2.37496825 -0.89445391]]\ndW =  [[-0.10076895  1.40685096  1.64992505]]\ndb =  [[0.50629448]]\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(\"dA_prev = \", dA_prev)\n",
    "print(\"dW = \", dW)\n",
    "print(\"db = \", db)"
   ]
  },
  {
   "source": [
    "### 6.2 - 线性激活向后传播\n",
    "\n",
    "为了帮助你实现linear_activation_backward，我们提供了两个后向函数：\n",
    "\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**Exercise**: Implement the backpropagation for the *LINEAR->ACTIVATION* layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sigmoid:\ndA_prev =  [[ 0.11017994  0.01105339]\n [ 0.09466817  0.00949723]\n [-0.05743092 -0.00576154]]\ndW =  [[ 0.10266786  0.09778551 -0.01968084]]\ndb =  [[-0.05729622]]\nrelu:\ndA_prev =  [[ 0.44090989 -0.        ]\n [ 0.37883606 -0.        ]\n [-0.2298228   0.        ]]\ndW =  [[ 0.44513824  0.37371418 -0.10478989]]\ndb =  [[-0.20837892]]\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation=\"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \", dA_prev)\n",
    "print (\"dW = \", dW)\n",
    "print (\"db = \", db)\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \", dA_prev)\n",
    "print (\"dW = \", dW)\n",
    "print (\"db = \", db)"
   ]
  },
  {
   "source": [
    "### 6.3 - L层神经网络模型反向传播\n",
    "\n",
    "现在您将实现整个网络的后向传播功能. Recall that when you implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the `L_model_backward` function, you will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$.\n",
    "\n",
    "** Initializing backpropagation**:\n",
    "To backpropagate through this network, we know that the output is, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "You can then use this post-activation gradient `dAL` to keep going backward. As seen in Figure 5, you can now feed in `dAL` into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula : \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "For example, for $l=3$ this would store $dW^{[l]}$ in `grads[\"dW3\"]`.\n",
    "\n",
    "**Exercise**: Implement backpropagation for the *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    # 第L层\n",
    "    dAL = -np.divide(Y, AL) + np.divide(1-Y, 1-AL)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "\n",
    "    # 从L-1层依次反向传播\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)], current_cache, \"relu\")\n",
    "        grads[\"dA\"+str(l+1)] = dA_prev_temp\n",
    "        grads[\"dW\"+str(l+1)] = dW_temp\n",
    "        grads[\"db\"+str(l+1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dW1 =  [[0.41010002 0.07807203 0.13798444 0.10502167]\n [0.         0.         0.         0.        ]\n [0.05283652 0.01005865 0.01777766 0.0135308 ]]\ndb1 =  [[-0.22007063]\n [ 0.        ]\n [-0.02835349]]\ndA1 =  [[ 0.          0.52257901]\n [ 0.         -0.3269206 ]\n [ 0.         -0.32070404]\n [ 0.         -0.74079187]]\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print(\"dW1 = \", grads[\"dW1\"])\n",
    "print(\"db1 = \", grads[\"db1\"]) \n",
    "print(\"dA1 = \", grads[\"dA1\"])"
   ]
  },
  {
   "source": [
    "### 6.4 - 更新参数\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, lr):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)] -= lr*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)] -= lr*grads[\"db\"+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "W1 =  [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n [-1.0535704  -0.86128581  0.68284052  2.20374577]]\nb1 =  [[-0.04659241]\n [-1.28888275]\n [ 0.53405496]]\nW2 =  [[-0.55569196  0.0354055   1.32964895]]\nb2 =  [[-0.84610769]]\n"
    }
   ],
   "source": [
    "# 测试一下\n",
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "print(\"W1 = \", parameters[\"W1\"])\n",
    "print(\"b1 = \", parameters[\"b1\"])\n",
    "print(\"W2 = \", parameters[\"W2\"])\n",
    "print(\"b2 = \", parameters[\"b2\"])"
   ]
  },
  {
   "source": [
    "\n",
    "## 7 - 总结\n",
    "\n",
    "恭喜您实施构建深度神经网络所需的所有功能！\n",
    "\n",
    "我们知道这是一个长期的任务，但前进只会变得更好。下一部分任务更容易。\n",
    "\n",
    "在下一个任务中，你将把所有这些放在一起来构建两个模型：\n",
    "- 双层神经网络\n",
    "- 一个L层神经网络\n",
    "\n",
    "实际上，您将使用这些模型来分类猫与非猫的图像！"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}