{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601273513643",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Python Basics with Numpy (optional assignment)\n",
    "\n",
    "欢迎来到第一个任务。本练习提供了对Python的简要介绍。即使您以前使用过Python，这也可以帮助您熟悉我们即将用到的的功能。\n",
    "\n",
    "**说明：**\n",
    "\n",
    "- 你将学会使用Python 3。\n",
    "- 除非明确告知，否则避免使用for循环和while循环\n",
    "- 不要修改单元格中的（＃GRADED FUNCTION [函数名称]）注释。如果改变，函数会失效。每个包含该注释的单元格仅包含一个函数\n",
    "- 编写完函数后，运行它下面的单元格来检查结果是否正确。\n",
    "\n",
    "**完成这项任务后，您将：**\n",
    "\n",
    "- 能够使用 iPython Notebooks\n",
    "- 能够使用numpy函数和numpy矩阵/矢量操作\n",
    "- 了解python “广播”的概念\n",
    "- 能够矢量化代码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks 是嵌入在网页中的交互式编码环境。您将在本课程中使用iPython Notebooks。您只需要在 ### START CODE HERE ### 和 ### END CODE HERE ### 注释之间编写代码。编写完后，可以通过按“SHIFT”+“ENTER”或单击Notebooks上方的“Run Cell”（用播放符号表示）来运行单元格。\n",
    "\n",
    "我们经常在注释中指定\"(≈ X lines of code)\"，告诉你需要写多少代码。这只是一个粗略的估计，所以如果你的代码更长或更短无需奇怪。\n",
    "\n",
    "**练习**：在下面的单元格中输入“Hello World”并运行下面的两个单元格。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1 - Building basic functions with numpy ##\n",
    "\n",
    "Numpy是Python科学计算中的主要软件包。它由一个大型社区维护（www.numpy.org）。在本练习中，您将学习 np.exp，np.log 和 np.reshape 等几个重要的 numpy 函数。因为未来的作业需要使用它们。\n",
    "\n",
    "### 1.1 - sigmoid function, np.exp() ###\n",
    "\n",
    "在使用 np.exp（）之前，您将使用 math.exp（）来实现 sigmoid 函数。然后你会明白为什么np.exp（）比math.exp（）更好一些。\n",
    "\n",
    "**练习**：构建一个返回实数x的sigmoid函数，并使用math.exp（x）作为指数函数。\n",
    "\n",
    "**提示**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$有时也称为 logistic function。它是一种非线性函数，不仅用于机器学习（Logistic回归），还用于深度学习。\n",
    "\n",
    "要引用特定包的函数，可以通过“包名.函数名”的方式来调用。运行下面的代码查看 math.exp（）示例。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9525741268224334"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "sigmoid(3)"
   ]
  },
  {
   "source": [
    "### 1.2 - Sigmoid gradient（梯度）\n",
    "\n",
    "正如在课堂上学到的，反向传播优化损失函数需要去计算梯度。现在让我们来编写第一个梯度函数。\n",
    "\n",
    "**练习**：完成sigmoid的梯度函数，用它去计算sigmoid相对于其输入x的梯度。公式是：\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "你通常会花两步去编写这个函数：  \n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.  \n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "  def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    计算sigmoid function函数相对于其输入x的梯度（也称为斜率或者导数）.\n",
    "    你可以将sigmoid function函数的输出存储到变量中，然后用它来计算梯度\n",
    "    \n",
    "    Arguments:\n",
    "    x -- 一个标量或者numpy数组\n",
    "\n",
    "    Return:\n",
    "    ds -- 你计算好的梯度\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1 - s)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds  "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "print(\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "在深度学习中有两个numpy函数经常使用：[np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) 和 [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape用于获取矩阵/向量X的形状（维度）。\n",
    "- X.reshape(...)用于将X重塑为其他维度。\n",
    "\n",
    "例如，在计算机科学中，图像是由三维形状数组来表示的$(length, height, depth = 3)$。但是，当你读取图像作为算法的输入时，需要将其转换为形状的矢量 $(length*height*3, 1)$。换句话说，就是你要将3D数组“展开”或重塑造成一维矢量。\n",
    "\n",
    "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
    "```python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- 请不要将图像的尺寸硬编码为常量。 Instead look up the quantities you need with `image.shape[0]`, etc. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "image2vector(image) = [[0.67826139]\n [0.29380381]\n [0.90714982]\n [0.52835647]\n [0.4215251 ]\n [0.45017551]\n [0.92814219]\n [0.96677647]\n [0.85304703]\n [0.52351845]\n [0.19981397]\n [0.27417313]\n [0.60659855]\n [0.00533165]\n [0.10820313]\n [0.49978937]\n [0.34144279]\n [0.94630077]]\n(18, 1)\n"
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "v = image2vector(image)\n",
    "print (\"image2vector(image) = \" + str(v))\n",
    "print(v.shape)"
   ]
  },
  {
   "source": [
    "### 1.4 - Normalizing rows\n",
    "\n",
    "我们在机器学习和深度学习中常用的另一个技术是正则化我们的数据. 它经常产生更好的性能，因为梯度下降在正则化之后收敛得更快。 在这里，正则化的意思是把x改成 $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it in part 5.\n",
    "\n",
    "\n",
    "**Exercise**: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = np.linalg.norm(x, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Divide x by its norm.\n",
    "    x = x / x_norm\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "normalizeRows(x) = [[0.         0.6        0.8       ]\n [0.13736056 0.82416338 0.54944226]]\n"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "source": [
    "### 1.5 - Broadcasting and the softmax function ####\n",
    "在numpy中有一个很重要的概念叫“广播”。它在不同形状的数组之间进行数学运算是非常有用的。有关广播的全部细节，您可以阅读官方的[broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**练习**：使用numpy实现softmax函数。当你需要对两个或更多的类进行分类时，您可以将softmax视为normalizing function。您将在本专业的第二课程中学习更多有关softmax的知识。\n",
    "\n",
    "**Instructions**:\n",
    "- $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix}$ \n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n}$,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: $  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x)\n",
    "    s = x_exp / x_sum\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[289.39585456   0.26389486   5.30046997   0.03571429   0.03571429]\n [ 39.16546994   5.30046997   0.03571429   0.03571429   0.03571429]]\n"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)  # 禁用科学计数法\n",
    "x = np.array([[9,2,5,0,0],\n",
    "                [7,5,0,0,0]])\n",
    "\n",
    "print(softmax(x))"
   ]
  },
  {
   "source": [
    "## 2) Vectorization（向量化）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "在深度学习中，需要处理非常大的数据集。为了确保计算高效，数据需要向量化。 For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]"
   ]
  },
  {
   "source": [
    "### 向量化前"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dot = 278\nouter = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\nmul = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n"
    }
   ],
   "source": [
    "\n",
    "# 计算点乘（内积）\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i]*x2[i]\n",
    "print (\"dot = \" + str(dot))\n",
    "\n",
    "\n",
    "# 计算叉乘（外积）\n",
    "outer = np.zeros(shape=(len(x1),len(x2)))\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "print (\"outer = \" + str(outer))\n",
    "\n",
    "# 计算对应乘\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "print (\"mul = \" + str(mul))\n"
   ]
  },
  {
   "source": [
    "### 向量化后"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dot = 278\nouter = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\nmul = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n"
    }
   ],
   "source": [
    "# 计算点乘（内积）\n",
    "dot = np.dot(x1,x2)\n",
    "print (\"dot = \" + str(dot))\n",
    "\n",
    "# 计算叉乘（外积）\n",
    "outer = np.outer(x1, x2)\n",
    "print (\"outer = \" + str(outer))\n",
    "\n",
    "# 计算对应乘\n",
    "mul = np.multiply(x1,x2)\n",
    "print (\"mul = \" + str(mul))"
   ]
  },
  {
   "source": [
    "### 2.1 Implement the L1 and L2 loss functions\n",
    "\n",
    "**Exercise**: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "**Reminder**:\n",
    "- 成本函数用于评估模型的性能。你的预测值($ \\hat{y} $)和真实值($y$)相差越大，你的成本函数就越大。在深度学习中，您可以使用梯度下降等优化算法来训练模型，并将成本降至最低。\n",
    "- L1 loss is defined as:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(y_hat, y):\n",
    "    loss = np.sum(np.abs(y - y_hat))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "L1 = 1.1\n"
    }
   ],
   "source": [
    "y_hat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1,0,0,1,1])\n",
    "print(\"L1 = \" + str(L1(y_hat,y)))"
   ]
  },
  {
   "source": [
    "**Exercise**: 实现L2损失的numpy向量化版本。实现L2损失有好几种方法，但你可能发现函数 np.dot() 更好用. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(y_hat, y):\n",
    "    loss = np.dot(y-y_hat, (y-y_hat).T)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "L2 = 0.43\n"
    }
   ],
   "source": [
    "y_hat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1,0,0,1,1])\n",
    "print(\"L2 = \" + str(L2(y_hat,y)))"
   ]
  }
 ]
}